%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this of into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  print, %% This option enables the default options for the
           %% digital version of a document. Replace with `printed`
           %% to enable the default options for the printed version
           %% of a document.
  Table,   %% Causes the coloring of tables. Replace with `notable`
           %% to restore plain tables.
  nolof,     %% `lof` Prints the List of Figures. Replace with `nolof` to
           %% hide the List of Figures.
  nolot,     %% `lot` Prints the List of Tables. Replace with `nolot` to
           %% hide the List of Tables.
  %draft, %TODO remove, place final instead
  11pt, % TODO disscus this
  oneside  %% twoside for print
  %% More options are listed in the user guide at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the a T2A font encoding
\usepackage[T1]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  %german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
%\usepackage{paratype}
%\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date          = \the\year/\the\month/\the\day,
    university    = mu,
    faculty       = fi,
    type          = mgr,
    author        = Karel Kubíček,
    gender        = m,
    advisor       = {RNDr. Petr Švenda, Ph.D.},
    title         = {Optimisation heuristics in randomness testing},
    TeXtitle      = {Optimisation heuristics in randomness testing},
    keywords      = {randomness testing, cryptoanalysis, block functions, stream functions, hash functions, problem optimization, metaheuristics},
    TeXkeywords   = {randomness testing, cryptoanalysis, block functions, stream functions, hash functions, problem optimization, metaheuristics},
}
\thesislong{abstract}{
    TODO
}
\thesislong{thanks}{
    TODO (collegues, friends, Marta, Petr, family)
}
%% The following section sets up the bibliography.
\usepackage{csquotes}
\usepackage[              %% When typesetting the bibliography, the
  backend=biber,          %% `numeric` style will be used for the
  style=numeric,          %% entries and the `numeric-comp` style
  citestyle=numeric-comp, %% for the references to the entries. The
  sorting=none,           %% entries will be sorted in cite order.
  sortlocale=auto         %% For more unformation about the available
]{biblatex}               %% `style`s and `citestyles`, see:
%% <http://mirrors.ctan.org/macros/latex/contrib/biblatex/doc/biblatex.pdf>.
\addbibresource{thesis.bib} %% The bibliograpic database within
                          %% the file `example.bib` will be used.
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{menukeys}

% cref, has to be loaded after hyperref
\usepackage{hyperref}
\usepackage{cleveref}
% for long equation wrap
\usepackage{wrapfig}
% ???
\usepackage{subcaption}

% tables
\usepackage{tabularx}
% colored cells (cellcolor)
\usepackage{colortbl}

% my colours
\usepackage{xcolor}

%% My own inputs:
% enabling new fonts support (nicer)
\usepackage{lmodern}
% better typeset of line ends and so (nicer)
\usepackage{microtype}
% code
\usepackage{minted}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}

% package to make bullet list nicer
\usepackage{enumitem}
\setitemize{noitemsep,topsep=3pt,parsep=3pt,partopsep=3pt}

% intendation
\usepackage{parskip}

% my own progress commands
\newcommand{\todo}[1]{TODO: \textit{#1}}
\newcommand{\rewrite}[1]{rewrite: \textit{#1}}
\newcommand{\reread}[1]{reread: \textbf{#1}}

% table colours
\newcommand{\fd}{\cellcolor{red!13}}
\newcommand{\fn}{\cellcolor{green!13}}

% make captions italic
\usepackage[format=plain,
            font=it]{caption}


\begin{document}

% English indentation, vertical, not horizontal
\setlength{\parskip}{5pt}
\setlength{\parindent}{0pt}

%% We will define several mathematical sectioning commands.
%\newtheorem{theorem}{Theorem}[section] %% The numbering of theorems
                               %% will be reset after each section.
%\newtheorem{lemma}[theorem]{Lemma}     %% The numbering of lemmas
%\newtheorem{corr}[theorem]{Corrolary}  %% and corrolaries will
                                %% share the counter with theorems.
%\theoremstyle{definition}
%\newtheorem{definition}{Definition}
%\theoremstyle{remark}
%\newtheorem*{remark}{Remark}

\chapter{Introduction}
\label{chap:introduction}

Cryptography is a field of computer science, used daily by all Internet users. Users connecting to HTTPS websites are using asymmetric cryptography to generate a symmetric key for encryption of the following communication. For such use-case, as well as various others, are used cryptographic primitives as public-key encryption functions, hash functions, block functions and stream functions. Current research of cryptoprimitives has a nontrivial goal. Design and develop algorithms, such that can hold against unknown threats as long as possible. Therefore cryptographers follow the standardised approach in cryptoprimitives development to ensure quality by a lengthy process of preparation, comparison and testing.

\section{Cryptoprimitives selection process}
\label{sec:crypto-sel-proc}

A standardisation committee reacts on inquiry for new or updated cryptoprimitive by issuing a competition for it. For example competitions during last 20 years were AES, which called for a symmetric block cipher, eSTREAM, for new stream ciphers, SHA-3 for hash function and CAESAR for authenticated ciphers. Those competitions last usually around five years. The first phase is collecting proposals, which are filtered in the second phase to the gradual selection of the winner candidate in the third. The winning solution became standard, and the developers will use it usually for at least ten years. For example, Data Encryption Standard was standardised in 1976, and Triple-DES modification surpassed it as a standard in 1999. Final successor Advanced Encryption Standard came in 2002, and it remains functional until today (2017).

\section{Cryptoanalysis}
\label{sec:cryptoanalysis}

During the second phase, proposals are analysed by cryptoanalysts. While proposing own design is possible for both skilled teams and almost cryptography beginners, the analysis is demanding process that needs both skill and time.

Cryptanalysis is expensive work, as the required cryptoanalyst skills are demanding. It consumes a significant amount of time to reject even proposals vulnerable to known threats. Therefore, a software tool capable of rejecting vulnerable proposals can save cryptoanalyst's time. Moreover, the cryptanalyst can use the tool for quicker dive into the candidates' specifications. Such tool can, for example, identify optional parts and show weak or even threat parts of the proposed function.

\subsection[Cryptorimitives properties]{Overview of expected properties of cryptorimitives}
\label{subsec:crypto-prop}

Field of the analysis, which is this thesis aiming for, are hash functions, block ciphers and stream ciphers (further referred as functions). There are several properties, formulated as a minimal set of conditions for these functions which has to be satisfied. If the function does not meet some property, it is considered weak, while satisfaction of all of the properties does not imply nonvulnerable function. Some of the properties are following.

Irreversibility without the encryption key: from given ciphertext, the attacker is not able to reconstruct any bit of plaintext with a probability higher than 50\,\%. If the probability were over 50\,\%, the ciphertext would reveal some information about the plaintext.

Strict avalanche criterion: every bit flip of plaintext changes around 50\,\% of output bits. Not satisfying this would allow an attacker to start with plaintext of zeroes and continuously modify single bits to get closer to given ciphertext.

Oracle, semantic security: even thou plaintext usually does not look like a random data, the ciphertext have to look so. If the output were distinguishable from random data, it would reveal information about the plaintext. This property is tested in our tool EACirc. However, the previous properties are tested as well, yet not directly.

\section{Cryptoanalysis automisation}
\label{sec:cryptoanalysis-automisation}



\subsection[Optimization problem for a cryptoprimitive]{Building an optimization problem from property of cryptoprimitives}
\label{subsec:opti-crypto}


\section{Statistical testing}
\label{sec:stat-testing}

Testing of randomness is a common area of statistics research. Thus statisticians have developed a rigorous theory of probability, and they also produced practical tools, that can test randomness. These tools are used even in cryptography to analyse function for properties from the previous section.

\subsection{Statistical batteries}
\label{subsec:stat-batt}

The statistical battery is a set of simple tests, which are analysing simple properties of the binary stream. For example, a monobit test counts an amount of zeros and ones in the input data, which should be similar in the case of the uniform stream. The tests can analyse the stream for patterns, which should not be present in random data.

The design of the tests accords to some statistical property. As the theory behind the statistical test is complex, the development of the test is difficult. The test set is fixed, so they cannot adapt to the input. The main advantage of EACirc is an ability to adapt to tested data. The tool itself learns on given data, and after this learning process, the output is randomness test for this specific type of data.

Best known statistical batteries are (chronologically by time of development) NIST STS, Diehard and Dieharder and TestU01. While NIST STS is already completely surpassed by its successors, statisticians still consider it as a golden standard of statistical testing. The EACirc performs better than NIST STS, is comparable with Dieharder and currently perform worse than TestU01. The ultimate goal of implementation of metaheuristics would be surpassing Dieharder and compete with TestU01.

\subsection{EACirc}
\label{subsec:eacirc}

\section{EACirc computation loop}
\label{sec:eac-comp}

\section{EACirc's potential}
\label{sec:eac-potential}

\subsection{Individual representation}
\label{subsec:eac-indiv-repres}

\subsubsection{Circuit}
\label{subsubsec:eac-indiv-repres-circuit}

\subsubsection{Polynomials}
\label{subsubsec:eac-indiv-repres-poly}

\subsection{Speed-up of computation}
\label{subsec:eac-speedup}

\subsection{Evaluation}
\label{subsec:eac-eval}

\subsection{Learning process}
\label{subsec:eac-learning}

\section{Problem optimisation}
\label{sec:prob-opt}

\subsection{Metaheuristics}
\label{subsec:prob-opt-meta}

A computational problem is a task to find a feasible solution for given question. An optimisation problem is a task to find the feasible solution, which satisfies the criteria best. Usually, we formulate fitness function to compare how well are the criteria met. A heuristic is a problem specific technique for finding some an approximative solution, where exact techniques fail. Finally, the metaheuristic is technique abstracted from the problem so that it can be used for various optimisation problems.

Metaheuristics are applicable for optimisation problems if a programmer can provide three crucial parts. First is an instance of solution for the problem (despite the quality). Second is a function to vary the solution, called neighbourhood function. Third and usually the most important is the fitness function, which measures the quality of the solution. Metaheuristic uses these parts to follow the landscape of the problem, and they try to find better solutions during the time.

The main advantage of metaheuristics is that they are well known, compared together in many studies (see 849 references in Metaheuristics book [1]) that also provided tips for the implementation.

\subsection{ANN}
\label{subsec:prob-opt-ann}

\subsection{...}

\chapter{Current approaches in problem optimization}
\label{chap:optimisation}

The number of candidate solutions inspected simultaneously splits metaheuristics into two categories: single-solution based metaheuristics and population-based metaheuristics.

The main advantage of having multiple concurrent solutions is handling population diversity. Diversity can help avoid ending in local optima and together with other techniques can produce better individuals faster. However, maintaining the diversity of the population is generally difficult, if the fitness does not increase continuously. An individual having a unique feature, which increases its fitness, is going to be quickly spread in next generations, suppressing other promising, but not yet so well performing other individuals.

The initial version of EACirc tended to use the advantages of the genetic programming, which is a population based metaheuristic. Encoding the candidate solutions as a simulated electronic circuit allowed operations for mutation and crossover. Together with fitness evaluation was the optimisation process specified and used on cryptographic data, it was capable of distinguishing weak sources from truly random data~\cite{svenda2013towards}.

However, this approach was very complex and not enough understood. The sexual crossover did not have reasonable implementation and holding the diversity of the population was not analysed. Also the interpretation of the results difficult. The succeeding solution proposed in \cite{sys2014constructing} simplified the interpretation by using the population of only one individual. This solution allowed statistical postprocessing of fitness values and together with categories evaluator surpassed the previous approach in all tests~\cite{sys2014constructing}. The population with a single individual changed effectively the used method to iterated local search -- a single solution metaheuristic.

EACirc 4.0 simplified and optimised the computations by substituting GALib -- a genetic algorithm library by a custom implementation of the iterated local search. To question if iterated local search is the best approach, this work analyses the problem and tries other metaheuristics.

\section{Single-solution metaheuristics}
\label{sec:opt-single-sol}

\Cref{fig:opt-single-sol-overview} shows the classification of the improved variants of local search metaheuristics by authors of ParadisEO library.

\begin{figure}[H]
    \centering
    \footnotesize{
    \begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 7cm/#1,
        level distance = 1.5cm}, every node/.style = {shape=rectangle, 
        draw=white, align=center}]]
      \node {Strategies for improving local search}
        child { node [xshift=3 cm] {Iterate with different\\solutions} 
            child { node {Multi-start\\local search} }
            child { node [xshift=-0.8 cm] {Iterative local\\search, GRASP} }
        }
        child { node {Change landscape\\of the problem}
            child { node [yshift=-1.2 cm, xshift=-0.6 cm] {Change of the objective function\\or the input data}
                child { node {Guided\\local\\search} }
                child { node {Noisy method} }
                child { node {Smoothing\\method} } }
            child { node [yshift=-1.2 cm, xshift=0.6 cm]  {Use different\\neighbourhoods}
                child { node {Variable\\neighbourhood\\search} }
            } 
        }
        child { node [xshift=-3 cm] {Accept non-improving\\neighbours} 
            child { node [xshift=0.8 cm] {Simulated\\annealing} }
            child { node [xshift=-0.8 cm] {Tabu\\search} }
        };
    \end{tikzpicture}
    }
    \caption{Single-solution metaheuristics as specialized version of local search. The source:~\cite[figure~2.24, p.~125]{talbi2009metaheuristics}.}
    \label{fig:opt-single-sol-overview}
\end{figure}

\subsection{Local search}
\label{subsec:opt-single-sol-ls}

This metaheuristic is the very basic approach in problem optimisation. We begin with single candidate solution and we apply changes to this solution so we create a neighbour candidate. We compare fitnesses of these two solutions and we select the better candidate to the next generation. We iterate this approach until we find sufficient solution, or we tested limited amount of iterations.

The algorithm can be referred also as hill-climbing, as we with acceptance of an improving solution, we step up in the fitness space. This also means, that we can find only sub optimal solution -- a local optima.

Escaping local optima is an important phenomena, as local optima can be relative to tested data, or to the specific approach of the evaluation.

As EACirc is using an iterated version of local search, the application of local search would be straightforward. However, this would decrease the capabilities of EACirc do to omit of the learning process.

\subsection{Iterative local search}
\label{subsec:opt-single-sol-ils}

Iterative local search is and extension of local search. After defined amounts of iteration of basic local search, the tested data are changed. The computation on one set of data is called an \textit{epoch}, and the amount of iteration is called \textit{epoch length}.

During the epoch, the fitness progress is non-decreasing, however, after the end of each epoch, fitness is expected to drop. The final fitness has to be tested on new dataset, which applies for every metaheuristic. Therefore, we distinguish between the learning dataset and the testing dataset.

The \textit{epoch length} is crucial parameter of this metaheuristic. A short epoch length can be computationally more demanding due to higher data consumption, and the solution may not be able to learn anything on the data. Too long \textit{epoch} leads to \textit{overfitting}, a situation, when the individual is learning specific pattern of the current dataset over general patterns of all datasets.

The iterated local search is used in EACirc since~\cite{sys2014constructing}. Therefore it is used as an baseline of the single solution metaheuristics tested in this work. The overfitting process in EACirc was examined in chapter 7 of~\cite{ukropBcThesis}.

\subsection{GRASP}
\label{subsec:opt-single-sol-grasp}

The greedy randomised adaptive search procedure is splitting each iteration into two steps: a feasible solution construction by greedy algorithm and performing the local search using the generated solution.

This metaheuristics was abandoned as we did not found any greedy algorithm for circuit construction, such that would reflect tested data. Also it does not fit the iterative process of learning in \textit{epochs}.

\subsection{Multi-start local search}
\label{subsec:opt-single-sol-msls}

Multi-start local search is another approach how to surpass local optimum to find the global optimum. The metaheuristic starts the computation with $k$ randomly generated individuals that are concurrently optimised by local search.

This parallelism could increase the usage of the testing data in EACirc. The disadvantage is more difficult evaluation, as stated in chapter 5~\cite{ukropBcThesis}. It is a question, if we should evaluate the best, or the average, or random solution, or more solution simultaneously. Another possible advantage of multi-start local search can be parallelization of the computation. However, EACirc computation is already parallelized, neither on the level of single computation. Due to statistical interpretation, we have to repeat the computation process 1\,000 times.

We decided to not cover multi-start local search, as saving percents of computation time per more difficult interpretation is a bad trade-off.

\subsection{Guided local search}
\label{subsec:opt-single-sol-gls}

Guided local search is metaheuristic based on dynamic changing of the fitness function. If the metaheuristic got stuck in local optima, guided local search would change the fitness function so the individual can follow climbing.

The EACirc computation can get trapped in local optima mainly in two cases:

\begin{enumerate}[noitemsep,topsep=3pt,parsep=3pt,partopsep=3pt]
    \item the individual got lucky to guess current test set, but it will not be successful in next \textit{epoch}, or
    \item the individual is already successful due to finding feature of the tested data, but it could not get better.
\end{enumerate}

We do not need to fix the first case, as such individual would be replaced in next \textit{epoch}. However, we can use guided local search to improve individual from the second case. Our alternative fitness function can be designed as the sum of our current fitness and e.g. the number of connectors. Such fitness would simplify the solution, which would help us with the interpretation of the last individual. Alternatively, we can use completely different fitness function.

We decided to implement guided local search in EACirc, as it also represents single-solution metaheuristics with change of the objective function from \cref{fig:opt-single-sol-overview}.

\subsection{Noisy method}
\label{subsec:opt-single-sol-nois}

The noisy method is a another metaheuristic, which changes the fitness space to escape local optima. It is done by adding a random noise to the input data, as described in section 2.9.2 of~\cite{talbi2009metaheuristics}. At the beginning of the computation, the noise is the strongest, and it disappears during the computation, so the final \textit{epochs} can be evaluated on the original data.

In EACirc, our research goal is already distinguishing pseudo-random data from truly-random data. An additional random noise in the input would probably lead to even more difficult learning process. For example, the noise could flip bit on position, which is necessary for distinguishing between the data sources. Therefore was this method abandoned.
\subsection{Smoothing method}
\label{subsec:opt-single-sol-smooth}

Smoothing method is the another metaheuristic, which uses changes in fitness function to escape local optima. The method is based on smoothing the landscape of fitness function by computing a local average of all neigbours to given distance. If the fitness is overally increasing in one direction, but more zoomed observation shows small fluctuation, smoothing method smooths the fluctuation and allows locating the global optima.

\textit{Neigbours} in EACirc are defined as solutions obtained by mutation of the circuit. Available mutation are connecting or disconnecting a connector, or change of the Boolean function in the node. The neighbourhood of an individual in EACirc is enormous (over $2^{25}$ neigbours in distance 1 for basic EACirc setup), this method would not be possible due to computational requirements.

\subsection{Variable neighbourhood search}
\label{subsec:opt-single-sol-vns}

Variable neighbourhood search also change the landscape of the problem. However, it do not change the fitness function or its interpretation, but it defines other method for neigbour selection. IF the computation stucks in local optima, the neigbourhood method is changed. Or the neigbourhood can be changed periodically. This method does not guarantee escaping from local optima, as all neigbourhoods can contain only worse solutions.

It is difficult to specify different neigbourhood in EACirc, however, the method lead us to different idea. EACirc is heading two mutually exclusive problems. We would like to have circuit connected sparsely for easier interpretation of the final solution. However, we also want to have the circuit's input and output nodes connected. This can be provided by reduction of the searched neigbourhood.

If we already have a successful solution, we can inspect only solutions with fewer connectors. In contrast, if the circuit is not connected (therefore the circuit's fitness is equal to zero), we can inspect only individual with more connectors.

\subsection{Simulated annealing}
\label{subsec:opt-single-sol-sa}

Remaining two single-solution metaheuristics are based on acceptance of non-improving neighbours. Short term acceptance of worse solution can lead to escape from local optima and finding global optima in whole computation.

Simulated annealing metaheuristic is inspired by the physical process of annealing in metallurgy. Annealing is an iterative process of heating the metal to increase the size of the crystals, which reduces the probability of a defect. During the process, the metal is heated to lower and lower temperatures.

Simulated annealing uses variable temperature, which describes probability of acceptance of worse neighbour. This probability is decreasing during the computations, such that in the beginning, the likelihood of acceptance of worse solution is high, but at the end of the computation, only better solutions are accepted.

This metaheuristic is widely used, which was the reason for implementing it as a testing scenario for metaheuristics as an representation of group of single solution metaheuristics that accepts non-improving solutions. Its implementation in EACirc do not require any changes, only specification of this method.

However, this method is not specified for the iterative approach with learning \textit{epochs}. Also the acceptance of non-improving solution can lead to discarding good solution.

\subsection{Tabu search}
\label{subsec:opt-single-sol-tabu}

Like Simulated annealing, Tabu search can also accept non-improving solutions to escape from local optima. However, the decision whether to accept a non-improving solution is not based on probability, but on knowledge, if considered solution was visited. Therefore, Tabu search holds a memory of visited solutions, called \textit{tabu list}, and it would not use twice the same solution. This way, Tabu search avoids cycles during the optimisation process.

Application of Tabu search in EACirc has several issues. Firstly, how to store used individuals (memory usage may be a problem), which can be solved by storing only a hash of the solution. Secondly, the neighbourhood space of an individual is huge, as stated in \cref{subsec:opt-single-sol-smooth}. It is higher than the number of individuals tested during whole computation. Choosing twice the same solution is unlikely. Therefore we decided to try only Simulated annealing and leave the decision of Tabu search implementation on the result of Simulated annealing.

\section{Multi-solutions metaheuristics}
\label{sec:opt-multi-sol}

Multi-solutions metaheuristics are usually inspired by biological processes of some population. The group of evolutionary algorithms is inspired by process of biological evolution, that process a population of species and forms better individuals. The ant colony optimization and swarm intelligence algorithms are based on motion behaivour of colony of selected species.

Multi-solutions metaheuristics are usually more complex than single-solution metaheuristics. The complexity shows on many layers.

\begin{itemize}
    \item The implementation usually requires many configurable parameters, therefore it is more prone to programming mistake.
    \item The fine-tuning of these parameters can be sensitive to the selected problem.
    \item The algorithm needs more computational resources due to overhead with maintenance more complex method.
\end{itemize}

These disadvantages may be paid of. The population together may prevent the algorithm stucking in local optima, if the diversity of the population is maintained well. Another straightforward advantage is application in similar process, that is the metaheuristic inspired by.

\subsection{Evolutionary algorithms}
\label{subsec:opt-multi-sol-ea}



\subsection{Ant colony optimization}
\label{subsec:opt-multi-sol-aco}

The ant colony optimization is an instance of swarm intelligence metaheuristics. This metaheuristic imitates the cooperative behaviour of ants. Every ant has simple goal of collecting food and the shared goal is to find shorted path between the food and the nest. The motion of the ants is coordinated by smell and a pheromone.

\begin{itemize}
    \item Every ant spreads pheromone on its trail.
    \item Ant prefer to take path with more pheromone.
    \item The pheromone evaporates over time.
\end{itemize}

The common application of ant colony optimization is in shortest path algorithms. Literature states many applications in traveling salesman problem \todo{cite}. However, the metaheuristic can be applied also in combinatorial problems, as the problem representation can be encoded to the graph structure and the shorted path algorithm may solve them. An example of such application can be formula satisfiability problem (SAT)~\cite{moritz2010solving}.

The problems solvable by ant colony optimisation are usually problems, where the solution is constructed from \textit{building blocks}. The pheromone can be associated to the block and it can describe probability of using the block in final solution. Such problem instances are used in EACirc's polynomial representation.

We wanted to use ant colony optimisation on polynomials, however, we did not find suitable library for \texttt{Python} or \texttt{C++}, such that would allow evaluation of whole constructed individual. Libraries like \texttt{ACO-Pants}~\cite{acoPants} are specialized for path finding, where every edge have to be weighted, while we would need to evaluate the whole path.

\subsection{Scatter search}
\label{subsec:opt-multi-sol-scatter}
\subsection{Other swarm intelligence methods}
\label{subsec:opt-multi-sol-si}
\section{Other methods}
\label{sec:opt-other}

% what ML problem specification heads-to?
% ML is well studied, but the main problem is selection of the method, fine-tuning the parameters and setting up the features
% 

\subsection{ANN}
\label{subsec:opt-other-ann}

% Introduce the idea - look for some thesis using btw. ANN
%   inspired by hebbian learning
%   backpropagation: Werbos 1975
%      gradient descent (optimisation method for backpropagation)
% Cite usage - find commonly cited papers, go trough them quickly
%   images
%   curves (signals)
%   text (selection of features)
% No-one tried to analyse binary data
%   feature selection is not straightforward, as it is specific to analysed data and we would have to learn even the process of feature selection.
%       EACirc: the circuit can be viewed as a feature selector, where the features are used afterwards for statistical testing (in the $\chi^{2}$ test)

The artificial neural networks (ANN) are classification optimisation method inspired by process of learning of brain and neural system. The initial idea was inspired by research of Donald Hebb~\todo{Cite: Hebb, Donald (1949)}. The Organization of Behavior. New York: Wiley.}, who described the process of learning of neurons of living beings. Trivial nonsupervised learning methods were based on Hebbian learning. Further Developement came with invention of backpropagation algorithm by Paul Werbos~\todo{Cite: Werbos, P.J. (1975). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences.}. The last rapid increase of usability of artificial neural networks came in 2006 with significant speed-up both of algorithm and its application on GPUs, which allowed deeper networks and more analysed data due to parallelization, leading to rapid progress in scalability and usability of artificial neural networks.
% deeper introduction to ANN can be found in book http://www.deeplearningbook.org

The artificial neural network consists of purpose specialised layers. The first layer is called visible layer and its input is the data for classification. Then there may be many hidden layers, that sequentially process output of previous layers. The final is output layer, which decides the classification of the data. Each layer consists of neurons. Each neuron take as input the output from all neurons of previous layers, and these outputs are multiplied by weight for each connection. The output is process by an activation function, which ensures normalisation of the output. There are many types of activation function, and each layer can use different activation function. The most used activation function is sigmoid function, with input from real number and output is usually from the interval $(0, 1)$.

\todo{citations: }
The artificial neural networks were used to solve various problems in classification. The most common are application in image processing, signal processing, text data processing and in artificial intelligence systems.

In cryptography, we have seen many application of ANN. For example researches from Google Brain showed experiment where they let neural network develop their own encryption for communication, while another neural network was intercepting the communication and attempting to decrypt it~\cite{abadi2016learning}. 
We have not found any application of ANN in classification of binary data. 

\subsection{Other statistical classification methods}
%https://en.wikipedia.org/wiki/Statistical_classification

% SVM
% kNN
% Decition trees (C4.5 algorithm https://en.wikipedia.org/wiki/C4.5_algorithm)
% Bayesian netowrks
% ...



\chapter{Experiment methodology}
\label{chap:method}

The primary testing goal of EACirc is to assess randomness of ciphertext of cryptographic functions. As most of the cryptographic functions are in a scope of automated analysis semantic secure, EACirc can identify non-randomness only for reduced functions. The functions can be reduced in a number of rounds, by processing selected plaintexts, and by selecting only specific bits of the output. As the analysis of reduced versions of cryptographic functions is the main goal of the EACirc project, it is also selected as the testing scenario for comparison of analysed metaheuristics.

The approach for evaluation is \todo{briefly?} described in \cref{subsec:eacirc}. The tested data were produced from 16 different cryptographic functions and pseudo-random data generator PCG~\cite{pcgGen} as reference data \todo{why is this better?}. These functions are reduced to the edge of distinguishing capability of EACirc, such that the EACirc is capable of distinguishing function reduced to $i$ rounds, but it is not able to distinguish $i+1$ rounds\footnote{Further is this round setting referred as "an edge".}. Usually we test also $i-1$ and $i+2$ rounds, for broader comparison of the results with state of the art of cryptoanalysis.

\section{Used data}
\label{sec:method-data}

Previous works on EACirc analysed candidates of cryptoprimitives competitions~\cite{estreamMgrThesis,sha3Thesis,ukropMgrThesis}. These works extended EACirc by many functions sharing the same interface. This approach was effective for broad analysis of the competition candidates, but it does not test well known cryptographic functions like AES and DES.

Therefore, for the comparison with other randomness testing tools and other research papers on semantics security, we decided select new test set of cryptographic functions. We chose the finalist of competitions SHA-3 (BLAKE, Grostl, JH, Keccak and Skein) and eSTREAM (HC-128, Rabbit, Salsa20, SOSEMANUK and Grain), and above that, we implemented other block (AES, DES, 3-DES and TEA) and stream (RC4) functions.

\subsection{Reimplementation of stream handling in EACirc}
\label{subsec:method-data-streams}

The implementation of SHA-3 and eStream candidates in EACirc was called \todo{projects}. The projects were handling everything related to data generation, specifically IV, key and plaintext. Every project had its own implementation of counter plaintext generator, strict avalanche criterion generator and so on.

For the implementation of the new project for block ciphers, we decided to reimplement data generation in more variable and modifiable way. For this, we decided for data manipulation via streams, whose can be nested into each other. Each stream implements simple interface with constructor and function \texttt{next}, which returns next test vector. The stream is specified only by the output size of the test vector.

The purpose of stream is unification of all data manipulation in the whole project, independently on the used cryptographic function. All streams inherit from abstract class \texttt{stream} with minimalistic interface. The streams have to specify its \texttt{size} and they have to implement method \texttt{next()}, which returns next test vector. We have currently many types of streams in EACirc.

\begin{itemize}
    \item Data sources streams, that are used as an input to cryptographic functions or as reference data. Example of such streams are:
    \begin{itemize}
        \item input streams: true and false bits generators, counter and multiple variants of the strict avalanche criterion generator, and
        \item PRNG's PCG32 and Mersenne Twister. \todo{cite?}
    \end{itemize}
    \item Projects of cryptographic functions. Namely project eSTREAM and SHA-3.
    \item Postprocessing streams are applied to data from cryptographic functions for more advanced experiments. They allows us analysis of selected bits of the function.
\end{itemize}

The streams are completely configurable via \texttt{JSON} configuration file. As the streams can be nested, the structure is described as a \texttt{JSON} tree. This updates changed the structure of the configuration file, therefore the configuration files are not backwards compatible. Nevertheless, the configuration is described and showed on examples in the project's documentation~\cite{EACirc-wiki-streams}.

All these changes significantly simplified the codebase, unified many implementations of the same functionality and broadened the experiment capabilities of EACirc. Therefore it was the main feature for EACirc~4.1 release.

\subsection{Implementation of functions}
\label{subsec:method-data-funcs}

As the source of AES, DES and 3-DES implementations was used educative repository~\cite{cryptoFunc}. The author of this repository verifies the encryption to the test vectors to ensure correctly working implementation. The only modification of the code is a limitation of the functions in the number of rounds.

TEA implementation is based on Wikipedia code~\cite{teaWiki}, and it was already implemented in EACirc before. The implementation was simplified and reduced due to changes in the interface. It is also reduced to selected amount of rounds.

RC4 is also based on the educative repository~\cite{cryptoFunc}, but as RC4 has no round structure, therefore, the code is unchanged.

All these functions together with the new interface of new block project and test vectors processing algorithm were released in minor version EACirc 4.1.


\subsection{Data generator tool}
\label{subsec:method-data-generator}


\subsubsection{?Testing correctness of the solution?}
\subsubsection{?Publication of the generator tool?}


\section{Methodology for specific optimisation methods}
\label{sec:method-spec}

As the tested scenarios vary not only by optimisation method but by used tools as well, the methodology changes for given metaheuristics. The base of the experiment was used for single-solution metaheuristics, as they were implemented directly to the EACirc and they inherited the experiment environment of our tool.

For direct inter-method comparison, we need to use the same data for testing. Therefore, all used data from the \todo{Usable testbed} are generated from the same implementation. A unified source of data was a significant demand that led to an implementation of the Generator tool \todo{described later in ... reference to part of the thesis with Generator}.

\subsection{Single-solution methodology}
\label{subsec:method-spec-ss}

\subsubsection{Statistical evaluation}
\label{subsubsec:method-spec-ss-stat}

The fitness of an individual is a $p$-value of two sample $\chi^{2}$ test, which tests, if the output bytes per source have same distribution. For a well-working individual on a weak source of data, the fitness is close to 1. As the quality of the individual decreases, the fitness decreases as well. However, completely random guess like coin tossing has the $p$-value uniformly distributed on the interval (0, 1).

As the $p$-values are uniformly distributed for random guess, getting high $p$-value can imply two cases. Either the $\chi^{2}$ test rejected the hypothesis and scored high confidential output, or the tested data, or in case of random guess, we were lucky and obtained high value from uniform ditribution on interval $(0, 1)$. Therefore, we avoid interpretation of a single $p$-value, we collect an $p$-value per every \textit{epoch}, and we process these $p$-values to Kolmogorov-Smirnov test. It analyses the uniformity of usually 300 $p$-values. This test is using critical value $\alpha = 1\,\%$, therefore it rejects the null hypothesis that tested data seems nonrandom in 1\,\% of runs. To avoid such case, we run the experiment with the same settings 1\,000 times and we analyse the rejection rate given as the amount of failed KS tests over 1000. If it is around 1\,\%, the data seem to be random, while in it is much higher, the experiment rejected randomness hypothesis of tested data.

This approach is vastly increasing the amount of data needed for the computation, as well as the overall run-time. This method is a trade-off for the success rate of EACirc, as different evaluators do not need statistical evaluation. Hence they need much fewer data; however, such evaluators are slightly worse based on results from~\cite{svenda2013towards}.

When we consider basic setup for single-solution metaheuristics, EACirc consumes 2.4\,GB of ciphertext of tested function \cref{fig:dataUsage}. The runtime of the experiment on MetaCentrum varies from 4 to 20 hours on a single core.

%http://tex.stackexchange.com/questions/57732/placing-figures-inside-a-two-column-document 3rd answer
\begin{figure*}[t]
    \begin{equation*}
        \begin{aligned}
    \Sigma = 1\,000 \;
             \frac{\textnormal{runs}}
                  {\textnormal{experiment}}
             \cdot
             \left(
             \frac{30\,000 \frac{\textnormal{generations}}{\textnormal{run}}}
                  {100 \; \frac{\textnormal{generations}}{\textnormal{test set}}}
             \cdot
             1\,000 \;
             \frac{\textnormal{vectors}}
                  {\textnormal{test set}}
             \cdot
             8 \;
             \frac{\textnormal{bytes}}
                  {\textnormal{vector}}
             \right) \approx\\
             \approx 2,4 \textnormal{\,GiB per experiment}
        \end{aligned}
    \end{equation*}
    \caption{The amount of data analyzed by EACirc for a single configuration of randomness testing experiment.}
    \label{fig:dataUsage}
\end{figure*}


\subsubsection{Computation automisation}
\label{subsubsec:method-spec-ss-auto}

As EACirc needs repeated computation on MetaCentrum grid infrastructure, the automation for binary deployment, computation distribution, aggregation of the results and their postprocessing are required.

The scripts are executed in few following steps.

\begin{enumerate}[noitemsep,topsep=3pt,parsep=3pt,partopsep=3pt]
 \item Firstly, a preprocessor creates necessary configuration files for the experiment.
 \item Secondly, MetaCentrum scripts distribute the computation among the grid infrastructure.
 \item Thirdly, after the computation finishes the postprocessor script aggregate the results. 
 \item Finally, the table generator script generates a human readable version of the results as well as database dump of the whole experiment. The whole process is showed in \cref{fig:scripts}.
\end{enumerate}

The EACirc project formelly used tool Oneclick created by Lubomír Obrátil~\cite{obratilBcThesis}. The Oneclick was a single tool for both preprocessing and postprocessing phase, and the run was parametrized. However, this approach was programmed for BOINC infrastracture and it is outdated for the MetaCentrum. The advantages of new scripts are: 

\begin{itemize}
    \item they are single purpose, easier to modify one without affecting other,
    \item they are written in bash (running scripts for MetaCentrum scheduler) and Python (processing), so they do not need to be compiled,
    \item they maintained up-to-date for the newest EACirc,
    \item they are faster, as MetaCentrum computation do not require previously used redundancy procedures.
\end{itemize}

These scripts are expected to be modified by individual researchers for the needs of their experiments, so the scripts are stored in GitHub repository~\cite{eaUtils}, and users are supposed to fork them. The repository also contains \texttt{readme.md} file, with specifies the first steps with computations on MetaCentrum with these scripts. Every script then contains documentation in the code (concerning expected modifications for individual purposes), and when it is run without arguments, it prints out a help message.

\todo{rotate to save space?}

\begin{figure}[H]
    \centering
    \tikzstyle{block} = [draw, rectangle, minimum height=3em, minimum width=3em]
    \tikzstyle{el} = [draw, ellipse, minimum height=3em, minimum width=3em]
    \tikzstyle{cl} = [cloud, cloud puffs=10,cloud puff arc=120, minimum height=3em, minimum width=3em]
    \begin{tikzpicture}[>=stealth, auto, node distance=3cm, every text node part/.style={align=center}]
        \node [el] (e) [] {\bf{experiment idea}};
        \node [block] (pre) [below of = e,yshift=0.5cm] {\bf{preprocessor}};
        \node [block,text width=3.5cm] (run) [below of = pre,yshift=-0.5cm] {\vspace{0.1cm}{\bf runner scripts} \texttt{run\_experiment.sh} runs for every config file script \texttt{single\_job.sh}};
        \node [block] (eac) [below of = run] {EACirc};
        \node [block] (pos) [below of = eac,yshift=0.6cm] {postprocessor};
        \node [block] (gen) [below of = pos,yshift=0.6cm] {table generator};
        \node [el] (res) [below of = gen,yshift=0.6cm] {readable results};

        \draw [->] (e) -- node [right] {researcher modifies\\general setting} (pre);
        \draw [->] (pre) -- node [right] {produces config for\\all combinations\\of function and round} (run);
        \draw [->] (run) -- node [right] {distributes computations\\on MetaCentrum grid} (eac);
        \draw [->] (eac) -- node [right] {results are returned\\to the frontend server} (pos);
        \draw [->] (pos) -- node [right] {aggregates the results\\and computes the statistics} (gen);
        \draw [->] (gen) -- node [right] {generates human readable tables\\and dump the results} (res);
    \end{tikzpicture}
    \caption{Execution of EACirc on MetaCentrum grid -- order and purpose of the scripts.}
    \label{fig:scripts}
\end{figure}

\paragraph{Preprocessor}

EACirc computation is based on a configuration file with specification and environment settings for particular experiment. As the test suite contains tens variations of parameters, it is necessary to automatize config file creation, so the user would not make a mistake during this process.

The scripts are written in Python and are maintained in variants per the scenario of tested functions.

\paragraph{Running scripts}

The scripts for distribution of computation among the MetaCentrum network are a result of Martin Ukrop's thesis~\cite{ukropMgrThesis}. They are continuously maintained in \texttt{EACirc-utils} repository~\cite{eaUtils} for current EACirc version.

As each experiment needs to be run multiple times for statistical evaluation, the scripts repeat the computation in a loop with different seeds usually for 1\,000 times. All runs are then collected by running scripts.

The current version of these scripts is also published on the GitHub~\cite{eaUtils}, and the forked repository is ready to run experiments in the usual way.

\paragraph{Postprocessor}

As the computations are repeated for statistical significance, the postprocessor script has to aggregate results from many produced files. It searches through output files and collects the results. Afterwards, it calculates an aggregated statistics of the experiment.

The output of the postprocessor is \texttt{JSON} dump of the results, whose are used both for human readable results generation and also storing the results for later usage.

\paragraph{Table generator}

The generator takes as an input the \texttt{JSON} dump of the results from the postprocessor. Currently, it only generates a \LaTeX table with highlighted results, but it can be extended for more complex manipulation with the results.

\subsubsection{Goal specification}
\label{subsubsec:method-spec-ss-goal}

\todo{Petr's comments are not corrected}

The ultimate goal of the experiments with metaheuristics is increasing detection capabilities of nonrandomness of tested datastream. For testing and measuring how a metaheuristic performs, we analyse swet of functions described in \cref{sec:method-data}. We increase the resolution of the comparison by testing cryptographic function reduced in number of rounds. However, reduction of the function to specific number of rounds is still a very steep, therefore the goal is hardly measurable on this scenario. The randomness of the data increases rapidly with the additional rounds of the cryptographic function. We would need functions with a low increase of entropy per round as a benchmark, but we did not observe such behaviour by any of the tested functions, nor our own design of benchmarking function do not behave that way [LUT cipher].

Less ambitious, but still useful goal can be an increase of the rejection rate of the null hypothesis per experiment. Rejecting only 10\,\% of runs lead to difficulties with the analysis of the output test. The higher is the rejection rate, the easier is the interpretation of found distinguisher. Also, if our rejection rate is close to critical value $\alpha$, then the higher rejection rate can overcome the edge value\footnote{For 1000 runs per experiment, this edge value is typically $2\cdot\alpha$. Such results can happen in less than 0.15\,\% of the experiments.} and we can reject the overall null hypothesis about the randomness of the source with higher confidentiality.

Another goal can be a decrease in the number of computations or the amount of used data. As we usually fix the epoch length to 100 iterations, the data usage and CPU time are directly related. Moreover, as the amount of used data is already high in this settings of EACirc, the time comparison is more valuable than consumed data comparison.

We can also analyse specific goals per metaheuristics. Both Variable neighbourhood search and Guided local search set a complementary target in simplification of the circuits for later analysis. This goal is harder to measure, so the analysis is done less rigorously.

The EACirc project is using adaptive learning. During the computation, the tested data are changing. We analysed over-learning of the individuals during the learning process, and the over-learning can be analysed as well. Therefore, decrease over-learning can be another goal for metaheuristics.

\subsection{Multi-solutions methodology}
\label{subsec:method-spec-ms}
\subsubsection{ANN methodology}
\label{subsubsec:method-spec-ms-aco}

\chapter{Experiment results}
\label{chap:res}

To verify the implementation, we perform sanity experiment comparing random data to random data. Such test gives us measured expected output in case that selected approach can not distinguish two tested streams. Expected value of such experiment differs per methodology and therefore is stated per every test scenario.

% SS methodology ref
\section{Single-solution metaheuristics}
\label{sec:res-ss}

%http://stattrek.com/online-calculator/binomial.aspx

Single solutions metaheuristics use evaluation described in [reference to Statistical evaluation subsubsect]. As we use critical value $\alpha=1\,\%$, the random-random experiment should reject randomness hypothesis in average $1\,\%$ of the runs. As we would like to have better confidentiality in this sanity experiment, we perform not only 1\,000 runs for this settings, but 100\,000 runs. Confidentiality for such run can be computed from the binomial distribution and obtaining value $1.2\,\%$ has probability 4.787\,\% for 1\,000 runs, while it is lower than 0.001\,\% for 100\,000 runs.

% todo https://texblog.org/2013/05/06/cleveref-a-clever-way-to-reference-in-latex/ cref multiple tables
Other results are summarised in tables (i.e. \cref{table:res-usable-ils}), where every row corresponds to one cryptographical function, and this function is reduced to rounds given by column of the table. A dash in the cell signalises that scenario was not tested. The number represents ratio of runs where KS test rejected the randomness hypothesis within 1\,000 runs. For indistinguishable data, values around measured ratio from the random-random experiment are expected. Results twice this value can happen in less then 0.15\,\% measures for randomly looking data. Therefore results over double of this measured critical value are considered as nonrandom.

The colour highlights our evaluation of the result. Red highlight signs that EACirc detected nonrandomness. Green highlight signs that EACirc has not found anything and from the notion of hypothesis testing, we cannot say anything about the randomness of data (we cannot accept the randomness hypothesis).

\subsection{Iterated local search baseline}
\label{subsec:res-ss-ils}

As was stated in \cref{subsec:eacirc} \todo{delete me, when you check this}, EACirc used Iterative local search as the heuristics since version 4.0. Therefore measuring this heuristics serves as a baseline, which allows direct comparison of other metaheuristics and also can be used to estimate comparison to previous EACirc results, whose were in this work verified.

\todo{fix the table}

\begin{table}[H]
\centering
\begin{tabular}{l|l l l l l l l l l l l}
Function\textbackslash{}rounds & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\ \hline
rnd\_rnd     & \fn{}0.01128 & --    & --    & --    & --    & --    & --    & --    & --    & --    & --   \\
AES          & \fd{}--    & \fd{}1.0   & \fd{}1.0   & \fd{}0.182 & \fn{}0.016 & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
BLAKE        & \fd{}1.0   & \fd{}0.107 & \fn{}0.014 & \fn{}0.01  & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
Grain        & \fd{}--    & \fd{}--    & \fd{}1.0   & \fn{}0.017 & \fn{}0.01  & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
Grostl       & \fd{}--    & \fd{}--    & \fd{}1.0   & \fn{}0.01  & \fn{}0.017 & \fn{}0.008 & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
HC-128       & \fn{}0.014 & \fn{}0.016 & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
JH           & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}1.0   & \fn{}0.008 & \fn{}0.014 & \fn{}--    & \fn{}--   \\
Keccak       & \fd{}--    & \fd{}1.0   & \fd{}1.0   & \fn{}0.017 & \fn{}0.017 & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
MD6          & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}0.748 & \fn{}0.01  & \fn{}0.012\\
Rabbit       & \fn{}0.019 & \fn{}0.007 & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
RC4          & \fd{}--    & \fn{}0.009 & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
Salsa20      & \fd{}--    & \fd{}--    & \fd{}1.0   & \fn{}0.01  & \fn{}0.013 & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
SINGLE-DES   & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}1.0   & \fd{}0.193 & \fn{}0.01  & \fn{}0.008 & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
Skein        & \fd{}1.0   & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
SOSEMANUK    & \fn{}0.011 & \fn{}0.013 & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
TEA          & \fd{}--    & \fd{}--    & \fd{}--    & \fd{}1.0   & \fn{}0.008 & \fn{}0.009 & \fn{}0.017 & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   \\
TRIPLE-DES   & \fd{}--    & \fd{}--    & \fd{}1.0   & \fn{}0.017 & \fn{}0.008 & \fn{}0.012 & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--    & \fn{}--   
\end{tabular}
\caption{Results of iterated local search as EACirc metaheuristics on usable testbed.}
\label{table:res-usable-ils}
\end{table}

From these results, we can have some general notes to the usable testbed.

\begin{itemize}
    \item The random-random experiment result is notably higher than 1\,\%. It is caused by known, but unresolved bug, described in the documentation~\cite{EACirc-wiki-bug}. This bug is a consequence of unconnected input to output in the circuit, so it is denoted \textit{connection bug}.
    \item Three rounds AES, one round BLAKE, eight rounds MD6 and four rounds DES are in the interpretation described in \cref{subsec:method-spec-ss} rejected, however rejection rate is less than 1\,\%, therefore we would not obtain a strong distinguisher. These functions take role as \textit{benchmarking functions}, as the different metaheuristics directly affect them.
\end{itemize}

We also performed other tests, which showed some aditional information.

\begin{itemize}
    \item The creation of distinguisher for one more round, or stronger distinguisher for e.g. three rounds AES is possible, however it needs manual fine-tuning to the data and more tested data. In study of TEA~\cite{kubicek2016new}, we showed distinguisher for 4 rounds TEA. Obtaining such result required different shape of the circuit due to TEA block length and 100 times more test vectors.
    \item The length of an epoch, which describes number of iterations performed on same dataset, was 100 for showed results. This setup seems to lead to overfitting, as was examined for example in section 7.1 of~\cite{ukropBcThesis}. We tried decreasing the epoch length to 20, which lead to increase of influence of the \textit{connection bug}, as the random-random experiment critical value was 0.03127. Such setup did not increased number of distinguished rounds for any function, but increased rejection rate of all the \textit{benchmarking functions}. However, only three rounds AES increased significantly to 0.982. Same behaviour appeared for shorter epochs of all metaheuristics.
\end{itemize}


\subsection{Simulated annealing}
\label{subsec:res-ss-sa}

The Simulated annealing was tested as a representative of single solution metaheuristics that can accept non-improving solution. The results are added to appendix in \todo{do so and refer table from here}, as they are very similar to \cref{table:res-usable-ils} with Iterative local search results.

The random-random experiment ended with 0.01681, which corresponds to higher influence of \textit{connection bug} for metaheuristics, which accepts non-improving neighbour. It probably also influenced \textit{benchmarking functions} results, so the higher rejection rate 0.293 of three rounds AES is not such significant improvement.

The \todo{ref SA table} is the final and the best result of 9 tested configurations. The simulated annealing uses 2 configurable variables that influences the learning process. They are initial temperature and cooling ratio and they both corresponds to probability of accepting an non-improving solution. We analysed 3 different cooling scenarios, every on 3 different initial temperatures and cooling ratio. The cooling scenarios are illustrated in figure \todo{figure with SA in one graph, do it wide but short} and the difference is based on behaviour on changed learning dataset. The tested variables were $(\mathit{initial~temperature, cooling~ratio}) \in \{ (500, 0.9), (100, 0.98), (50, 0.995) \}$. The results in \todo{link SA table} were obtained from blue temperature scheduling curve and the most conservative setup $\mathit{initial~temperature} = 500$ and $\mathit{cooling~ratio} = 0.9$.


\subsection{Guided local search}
\label{subsec:res-ss-gls}

Guided local search can escape local optima by change of the evaluation method. Our goal was set to escape not local optima, but unwanted solution with fitness equal to 0, which we refer as \textit{connection bug}. We penalise \textit{unconnected} solutions by the number of connectors, where more connected individuals have higher fitness. This proved to lead to more connected solutions faster. In contrast, solutions with fitness higher than 0.9 are penalised, if they have too many connectors. This hypothesis was also verified by the experiments. The both penalisation methods were tested independently and they both have similar role in increase of the results of \textit{benchmarking functions}. However, they are working especially well together, as they hold the circuits reasonable dense.

The penalisation for increasing connectivity can be referred as \textit{space exploration}, while the penalisation for reducing connectivity corresponds to \textit{solution exploitation}.% These phenomena are well defined in machine learning and we can 

\subsubsection{Analysis of the output circuits}
\label{subsubsec:res-ss-gls-circ-anal}

EACirc in this setup outperforms all other tested methods. However, we had set also secondary goal for this approach, which is simplification of the output circuit.

For this analysis, we had to prepare visualisation of the output circuit. The circuit is dumped to a file in \texttt{dot} language and transformed to image via graphviz visualisation tool \todo{cite}. We also perform \textit{pruning} of the circuit, which is done by BFS traversal of the circuit from the output node. This way, we can remove all connectors and nodes, that are not affecting the output. The \textit{pruning} works better for sparse circuits, where it can reduce the number of connectors by more than 50\,\%. Dense circuits can be pruned sometimes by less than 10\,\%.

Output from run using Guided local search created significantly sparser circuit, usually already with less than 50\,\% of connectors of Iterative local search runs. Pruning of such circuits ended with very simple circuits in overall 20 runs. These simpler circuits are easier for analysis, and they reveal additional information about the tested function. They may help cryptoanalyst with manual cryptanalysis, so they reveal potential weaknesses of the cryptographic function. The visualisation and pruning also helps our team with understasing of the EACirc computations. Dump of the final circuit can be also used as a minimalistic statistical test for the given data.

\todo{add circuits here -- probably unpruned ILS and pruned GLS}

\subsubsection{Unrelated fitness functions}

We have also analysed the original idea of Guided local search, such that we should change between unrelated fitnesses to help the metaheuristic to find better solution. We implemented another fitness method called \textit{weight evaluator}, which is not based on $\chi^{2}$ test. The output of the circuit is bit by bit evaluated as a guess about the source of the tested vector. These guesses are sum per all random test vectors and all test vectors from tested function. For 1\,000 test vectors per each group and 1 output byte, this means 8\,000 guesses per group. Random guesses should have similar amount of zeroes and ones. The ration is described by binomial distribution with same probability $p = 0.5$ for zero and one and $N = 8\,000$. We know this expected distribution of the guesses, so we can compute $p$-value of the measured data. Our analysis of this separate fitness function showed it is performing slightly worse than $\chi^{2}$ test, as it test only one criteria of the output, while $\chi^{2}$ test can detect also other sources of nonrandomness.

Concurrent usage of those both fitnesses resulted in significantly worse distinguishing capabilities than the first method described in this sub-section. For example, this approach was not able to distinguish three round AES.


\subsection{Variable neighbourhood search}
\label{subsec:res-ss-vns}

The goals and the approach of variable neighbourhood search is very similar to guided local search. The only difference is that guided local search changes density of the circuit trough changes of the fitness function, while variable neighbourhood search changes the density directly, as we inspect only neighbours, we would like to obtain.

If we have unconnected circuit, we try up to ten times addition of new connector until we do not have connected graph. In the opposite case, when we have good working individual, we examine only solutions with less connectors.

The variable neighbourhood search is performing better than iterative local search, however, it performs slightly worse in all cases than guided local search. Probably the limitation of the neigbourhood do not allow exploration of significantly better solution in case of well working solutions. Fine-tuning of the criteria, when we examine only sparser solutions can lead to better results.

\subsubsection{Analysis of the output circuits}
\label{subsubsec:res-ss-vns-circ-anal}

Same as in \cref{subsubsec:res-ss-gls-circ-anal}, we inspected the density of the circuits. Again, the circuits were sparser than circuits from iterated local search, however, they were significantly denser than circuits from guided local search runs. This may also support the hypothesis that limiting the neigbourhood is too strict, and disallow valuable changes in the learning process.

\subsection{Overfitting analysis}
\label{subsec:res-ss-overfitting}

\section{Multi-solutions metaheuristics}
\label{sec:res-ms}
\subsection{Bruteforce baseline}
\label{subsec:res-ms-bruteforce}
\subsection{Evolutionary algorithms}
\label{subsec:res-ms-aco}

\section{ANN}
\label{sec:res-ann}

% Implementation
%   preprocessing of input data
%       usage - learning data, testing data are separated
%   Keras library
% ANN
% CNN
% automation
% intepretation
% results
% cooperation with Italy
% future work: learn on AES r2, re-learn on aes r3

\section{Optimisation by grain fine-tuning selected metaheuristic}
\label{sec:res-finetuning}

% needs revision
Trying multiple metaheuristic is broad approach. However, as stated in no free lunch theorem, there is a tendency, than no single metaheuristic guarantee better results, while dedicated human work on analysing and fine-tuning the method can bring better results.

In~\cite{kubicek2016new}, we analysed deeply one single function -- Tiny Encryption Algorithm (\textit{TEA}). The aim of this analysis was finding best setup specific to tested function, so we can directly compare EACirc to other teams that were using genetics algorithm for testing randomness of TEA ciphertext.

Instead of broad analysis, we tried multiple settings mainly of the tested data. The best distinguishers were found for testing strict avalanche criterion. We also tested many other constants that specify the computation.

Intiialy, we observed worse results along all metaheuristics for TEA than are stated in the paper. Therefore, we analysed the the source of different behaviour. Due to this analysis, we found an implementation bug in EACirc 4 \todo{cite commit} that actively decreased the used size of the circuit by two layers. \todo{Updated results, other results were intouched?}.

This unintentional analysis showed that fine-tuning the dimensions of the circuit is crucial for observing the best performance.

\section{Inter-approach comparison}
\label{sec:res-comp}


\chapter{Related work}
\label{chap:relatwork}

\section{Statistical batteries}
\label{sec:relatwork-stat}

\subsection{Results}
\label{subsec:relatwork-stat-res}

\section{Compression algorithms}
\label{sec:relatwork-compress}

\section{?Search for papers on same topic?}
\label{sec:relatwork-paper}

\section{EACirc previous work}
\label{sec:relatwork-eac}

\chapter{Conclusion}
\label{chap:conclusion}

\section{Methodology}
\label{sec:conclusion-method}
\section{Results}
\label{sec:conclusion-results}
\section{Future work}
\label{sec:conclusion-future}

\chapter*{Acknowledgment}
\label{chap:ack}

Computational resources were provided by the CESNET LM2015042 and the CERIT Scientific Cloud LM2015085, provided under the programme "Projects of Large Research, Development, and Innovations Infrastructures".

Computational resources were supplied by the Ministry of Education, Youth and Sports of the Czech Republic under the Projects CESNET (Project No. LM2015042) and CERIT-Scientific Cloud (Project No. LM2015085) provided within the program Projects of Large Research, Development and Innovations Infrastructures.

We also acknowledge the support of Czech Science Foundation, the project GA16-08565S.


\chapter*{Appendix}
\label{chap:app}



\end{document}
